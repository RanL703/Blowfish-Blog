
[{"content":"","date":"14 March 2025","externalUrl":null,"permalink":"/tags/ai/","section":"Tags","summary":"","title":"AI","type":"tags"},{"content":"\rBloggeragent #\rAre you tired of manually creating blog posts about your GitHub projects? You\u0026rsquo;re in luck! Because today, we\u0026rsquo;re going to explore an amazing project that\u0026rsquo;s about to change the game: BloggerAgent! This incredible tool automatically generates blog posts from your GitHub repositories and publishes them to your Hugo blog site. Can you imagine having more time to focus on coding, while your blog stays up-to-date with your latest projects?\nProject Overview #\rSo, what exactly is BloggerAgent? In a nutshell, it\u0026rsquo;s a Python-based project that uses AI-powered content generation (via OpenRouter) or a fallback method to create blog posts about your GitHub repositories. But that\u0026rsquo;s not all - it also saves these posts to your Obsidian vault and syncs them with your Hugo blog, deploying changes seamlessly! The best part? It keeps track of processed repositories to avoid duplication, so you don\u0026rsquo;t have to worry about ending up with multiple posts about the same project.\nKey Features #\rHere are the standout features of BloggerAgent:\nü§ñ Automatically discovers your new GitHub repositories ‚úçÔ∏è Generates blog posts using AI (via OpenRouter) or a fallback method üìö Saves posts to your Obsidian vault üöÄ Syncs with your Hugo blog and deploys changes üîÑ Keeps track of processed repositories to avoid duplication What do you think? Are you as excited as I am about the possibilities? Technical Highlights #\rSo, what makes BloggerAgent technically impressive? For starters, it\u0026rsquo;s built using Python 3.6+, which means it\u0026rsquo;s stable, efficient, and easy to maintain. The project also uses OpenRouter for AI-powered content generation, which is a cutting-edge technology that\u0026rsquo;s still evolving! But what really sets it apart is its ability to integrate with Obsidian and Hugo, two popular tools in the developer community. Can you imagine the possibilities when you combine these powerful tools?\nUse Cases #\rNow, let\u0026rsquo;s talk about how you might use BloggerAgent in real life. Here are a few examples:\nAutomating blog posts: If you\u0026rsquo;re a developer who wants to showcase your projects on a blog, BloggerAgent can save you a ton of time and effort. Content generation: If you\u0026rsquo;re struggling to come up with content ideas, BloggerAgent\u0026rsquo;s AI-powered generation can help spark some creativity! Streamlining workflows: If you\u0026rsquo;re already using Obsidian and Hugo, BloggerAgent can help you streamline your workflow and reduce manual labor. Pro Tips or Implementation Details #\rTo get the most out of BloggerAgent, here are some pro tips:\nMake sure you have Python 3.6+ installed on your machine. Set up your GitHub account and generate a personal access token with the required scopes. Configure your Obsidian vault and Hugo blog settings in the .env file. Experiment with different OpenRouter API settings to fine-tune your content generation. Some other things to keep in mind:\nGitHub Token Setup: Follow the instructions in the README to set up your GitHub token. Path Configuration: Make sure you configure your Obsidian vault and Hugo blog paths correctly. Conclusion #\rAnd there you have it - BloggerAgent is an incredible project that can revolutionize the way you create and manage your blog content! With its AI-powered content generation, Obsidian and Hugo integration, and automatic deployment, it\u0026rsquo;s a game-changer for developers who want to showcase their projects. So what are you waiting for? Check out the repository, start exploring, and see how BloggerAgent can transform your blogging experience!\nCode Highlights #\rCheck out this awesome code snippet from the project:\nblog_generator.py #\rimport re\rfrom datetime import datetime\rfrom typing import Dict, List\rimport requests\rimport json\rimport os\rclass BlogGenerator:\rdef __init__(self, template: str):\rself.template = template\r// ... more code ... Project Details #\rGitHub Repository: BloggerAgent Created: 2025-03-14 Language: Python ","date":"14 March 2025","externalUrl":null,"permalink":"/posts/2025-03-14-bloggeragent/","section":"Posts","summary":"","title":"Bloggeragent","type":"posts"},{"content":"\rLinkedinagent #\rHook/Intro #\rHey there, tech enthusiast! Are you tired of manually sharing your GitHub projects on LinkedIn? Imagine having a personal assistant that automatically transforms your technical repositories into compelling LinkedIn content using cutting-edge AI technology! You\u0026rsquo;re in luck because the LinkedIn GitHub Agent is here to supercharge your LinkedIn presence!\nProject Overview #\rSo, what is this amazing tool all about? The LinkedIn GitHub Agent is a Python-based project that leverages Google\u0026rsquo;s Gemini-2.0-Flash model to create engaging posts from your GitHub repositories. It\u0026rsquo;s designed to save you time and effort by automating the process of sharing your projects on LinkedIn. But that\u0026rsquo;s not all - it also helps you drive engagement and establish your personal brand as a developer!\nKey Features #\rHere are the standout features of the LinkedIn GitHub Agent:\nAutomatic Repository Detection: Tracks your GitHub account for new repositories since the last run AI-Powered Content Generation: Creates engaging posts using Google\u0026rsquo;s Gemini-2.0-Flash model Professional Social Media Integration: Posts directly to LinkedIn through Ayrshare\u0026rsquo;s API Set-and-Forget Convenience: Just set up once and let it showcase your work! Smart State Tracking: Processes only new repositories to avoid duplicate posts Comprehensive Logging: Keeps detailed records of all activities for troubleshooting Technical Highlights #\rBut what makes this project technically impressive? For starters, it uses Google\u0026rsquo;s Gemini-2.0-Flash model, which is a state-of-the-art language model. It also utilizes Ayrshare\u0026rsquo;s API for seamless integration with LinkedIn. And, with its smart state tracking feature, you don\u0026rsquo;t have to worry about duplicate posts. The project is also well-structured and easy to set up, thanks to the helper batch files and clear documentation.\nUse Cases #\rSo, how can you use the LinkedIn GitHub Agent in real life? Here are a few examples:\nEstablish yourself as a thought leader: Share your projects and showcase your expertise to potential employers or clients. Drive engagement: Encourage discussions and feedback on your projects by sharing them with your LinkedIn network. Save time: Automate the process of sharing your projects and focus on what matters most - building amazing software! Pro Tips or Implementation Details #\rTo get the most out of the LinkedIn GitHub Agent, here are some pro tips:\nMake sure to configure your environment file correctly, including your GitHub personal access token, Google AI API key, and Ayrshare API key. Test the agent with a small repository to ensure everything is working as expected. Monitor the logs to troubleshoot any issues that may arise. Conclusion #\rIn conclusion, the LinkedIn GitHub Agent is an amazing tool that can supercharge your LinkedIn presence and establish you as a thought leader in your field. With its cutting-edge AI technology and seamless integration with LinkedIn, it\u0026rsquo;s a must-have for any developer looking to drive engagement and showcase their work. So, what are you waiting for? Check out the repo and start automating your LinkedIn posts today!\nCode Highlights #\rCheck out this awesome code snippet from the project:\nmain.py #\rimport os\rimport base64\rimport logging\rimport requests\rimport json\rimport datetime\rfrom datetime import timezone\rimport google.generativeai as genai\rfrom dotenv import load_dotenv\r// ... more code ... Project Details #\rGitHub Repository: LinkedInAgent Created: 2025-03-14 Language: Python ","date":"14 March 2025","externalUrl":null,"permalink":"/posts/2025-03-14-linkedinagent/","section":"Posts","summary":"","title":"Linkedinagent","type":"posts"},{"content":"","date":"14 March 2025","externalUrl":null,"permalink":"/posts/","section":"Posts","summary":"","title":"Posts","type":"posts"},{"content":"","date":"14 March 2025","externalUrl":null,"permalink":"/tags/python/","section":"Tags","summary":"","title":"Python","type":"tags"},{"content":"\rSpotiauto #\rHooked on Music Automation? #\rAre you tired of manually updating your Spotify playlists with the latest releases from your favorite artists? Well, you\u0026rsquo;re in luck! Imagine having a tool that automatically keeps your playlists fresh and up-to-date, so you can focus on what really matters - enjoying the music! Let\u0026rsquo;s dive into the amazing world of SpotiAuto, a Python-based automation tool that\u0026rsquo;s about to revolutionize your music streaming experience!\nProject Overview #\rSo, what is SpotiAuto? In a nutshell, it\u0026rsquo;s a game-changing script that synchronizes new tracks from your favorite artists to their dedicated playlists on Spotify. But that\u0026rsquo;s not all - it\u0026rsquo;s also incredibly easy to use and customizable, making it a must-have for any music lover! But what makes SpotiAuto truly special? Is it the fact that it\u0026rsquo;s free, open-source, and community-driven? Or is it the cutting-edge technology that powers it? Whatever the reason, one thing is clear: SpotiAuto is the perfect solution for anyone looking to take their music streaming to the next level!\nKey Features #\rHere are just a few of the standout features that make SpotiAuto so amazing:\nAutomated new release detection based on a configurable lookback period - how cool is that? Batch processing with smart rate limiting (3 tracks per batch) to prevent overwhelming the Spotify API Resilient error handling with an exponential backoff retry mechanism, because reliability matters Duplicate track prevention using playlist state tracking, so you don\u0026rsquo;t have to worry about duplicates Comprehensive logging with both file and console output, making it easy to track what\u0026rsquo;s happening OAuth-based Spotify authentication using Spotipy, for secure and seamless authentication Last run date tracking to optimize API calls and reduce unnecessary requests Technical Highlights #\rSo, what makes SpotiAuto technically impressive? For starters, it\u0026rsquo;s built using Python, one of the most popular and versatile programming languages out there! But that\u0026rsquo;s not all - it also features:\nA modular design, with separate components for Spotify API interactions (SpotifyClient) and automation workflow orchestration (PlaylistAutomation) Rate limiting protection to prevent 429 responses and ensure smooth operation Retry decorators with exponential backoff, for robust error handling Batch operations for efficient playlist updates Use Cases #\rBut how can you use SpotiAuto in real life? Here are a few examples:\nDiscover new music: Use SpotiAuto to create a playlist for your favorite artist and stay up-to-date with their latest releases! Create a playlist for a specific genre: Want to stay current with the latest hip-hop or electronic tracks? SpotiAuto can help you do just that! Automate your music discovery: Set up SpotiAuto to update your playlists regularly, and discover new artists and tracks without lifting a finger! Pro Tips or Implementation Details #\rSo, you want to get started with SpotiAuto? Here are a few pro tips to keep in mind:\nMake sure you have Python 3.6+ installed on your system Create a Spotify Developer application and obtain your Client ID and Client Secret Install the required dependencies, including Spotipy, PyYAML, and Tenacity Configure your SpotiAuto settings to your liking, including the lookback period and batch size Conclusion #\rAnd there you have it - SpotiAuto is an amazing tool that\u0026rsquo;s sure to revolutionize your music streaming experience! With its automated new release detection, batch processing, and resilient error handling, it\u0026rsquo;s the perfect solution for anyone looking to take their music to the next level. So what are you waiting for? Check out the repo, star it, and start automating your music playlists today! Don\u0026rsquo;t miss out on this game-changing opportunity to elevate your music streaming experience!\nCode Highlights #\rCheck out this awesome code snippet from the project:\nmain.py #\rimport logging\rimport logging.config\rimport yaml\rfrom pathlib import Path\rfrom spotiauto.automation import PlaylistAutomation\rdef setup_logging():\rPath(\u0026#34;logs\u0026#34;).mkdir(exist_ok=True)\rwith open(\u0026#34;config/config.yaml\u0026#34;, \u0026#39;r\u0026#39;) as f:\rconfig = yaml.safe_load(f)\r// ... more code ... Project Details #\rGitHub Repository: SpotiAuto Created: 2025-03-14 Language: Python ","date":"14 March 2025","externalUrl":null,"permalink":"/posts/2025-03-14-spotiauto/","section":"Posts","summary":"","title":"Spotiauto","type":"posts"},{"content":"","date":"14 March 2025","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"14 March 2025","externalUrl":null,"permalink":"/","section":"TheRanLBlog!","summary":"","title":"TheRanLBlog!","type":"page"},{"content":"\rAgentos #\rYou\u0026rsquo;re about to discover a game-changer in the world of voice assistants! Imagine having a modular, extensible, and highly customizable platform that can understand natural language and perform a wide range of tasks. Sounds like science fiction, right? Well, welcome to AgentOS, the revolutionary voice assistant platform that\u0026rsquo;s about to change the way you interact with technology!\nProject Overview #\rSo, what is AgentOS? In a nutshell, it\u0026rsquo;s a Python-based platform that allows you to create your own custom voice assistant with pluggable specialized agents. But that\u0026rsquo;s not all - it also comes with natural language understanding capabilities, which means it can translate user requests into actionable commands. The possibilities are endless!\nKey Features #\rHere are some of the standout features of AgentOS:\nVoice and text interface with conversational AI Modular agent architecture for extensibility Natural language understanding to translate user requests Todoist integration for task management File management capabilities Spotify integration for music control Twitter automation for social media management Blog post automation for content creation Electron-based UI for desktop integration But what makes AgentOS truly special is its modular design. You can easily add or remove agents to customize the platform to your needs. Want to integrate your favorite music streaming service? No problem! How about automating your social media posts? Easy peasy!\nTechnical Highlights #\rSo, what makes AgentOS tick? Python is the main language used, and the platform uses a combination of natural language processing (NLP) and machine learning (ML) to understand user requests. The modular architecture is designed to be highly extensible, making it easy to add new agents and features. But don\u0026rsquo;t worry if you\u0026rsquo;re not a tech expert - the setup instructions are easy to follow, and the README file provides all the information you need to get started.\nUse Cases #\rSo, how can you use AgentOS in real life? Here are a few examples:\nHome automation: Use AgentOS to control your smart home devices, play music, and set reminders. Task management: Integrate Todoist and use AgentOS to manage your tasks and projects. Content creation: Use AgentOS to automate your blog posts and social media updates. Music control: Use AgentOS to play your favorite music on Spotify and discover new artists. But the possibilities don\u0026rsquo;t stop there! With AgentOS, you can create your own custom voice assistant that fits your needs and lifestyle. What would you use it for?\nPro Tips or Implementation Details #\rWant to get the most out of AgentOS? Here are a few pro tips:\nMake sure to configure your environment variables correctly to ensure smooth operation. Experiment with different agents to find the ones that work best for you. Check out the Natural Language Agent setup instructions for detailed information on how to get started with NLP. Conclusion #\rAgentOS is an exciting and innovative project that has the potential to revolutionize the way we interact with technology. With its modular design, natural language understanding, and extensive features, it\u0026rsquo;s a must-try for anyone interested in voice assistants and AI. So, what are you waiting for? Head over to the GitHub repository, follow the setup instructions, and start building your own custom voice assistant today! The future of voice assistants is here, and it\u0026rsquo;s bright!\nCode Highlights #\rCheck out this awesome code snippet from the project:\nconfig.py #\r\u0026#34;\u0026#34;\u0026#34;\rAgentOS Configuration System\rThis module provides a centralized configuration for all AgentOS components.\rSettings can be overridden through environment variables or a .env file.\r\u0026#34;\u0026#34;\u0026#34;\rimport os\rimport platform\rimport logging\r// ... more code ... Project Details #\rGitHub Repository: AgentOS Created: 2025-03-12 Language: Python ","date":"12 March 2025","externalUrl":null,"permalink":"/posts/2025-03-12-agentos/","section":"Posts","summary":"","title":"Agentos","type":"posts"},{"content":"\rJarvis Welcome Automation #\rHey there, tech enthusiast! Are you ready to take your productivity to the next level? Imagine being able to launch your entire coding workspace with just a voice command! Sounds like something out of a sci-fi movie, right? Well, welcome to the Jarvis-welcome-automation project on GitHub, where this fantasy becomes a reality!\nProject Overview #\rSo, what exactly is this project all about? In a nutshell, it\u0026rsquo;s a Windows voice assistant that lets you launch your coding workspace with a simple voice command. But that\u0026rsquo;s not all - it also comes with some amazing features like snarky AI responses and text-to-speech feedback. Can you imagine having a conversation with your computer like you would with a friend?\nKey Features #\rHere are some of the standout features of this project:\nVoice activation with a customizable wake phrase - because who doesn\u0026rsquo;t want to feel like a boss? Launches your coding workspace via a shortcut - no more tedious clicking and typing! Snarky AI responses using Gemini (online) or Ollama (offline) - because a little humor never hurt anyone! Text-to-speech feedback - so you can stay focused on your code without having to constantly check your screen Technical Highlights #\rSo, what makes this project technically impressive? For starters, it\u0026rsquo;s built using Python 3.11, which is the latest and greatest version of the language. It also uses PyAudio for audio processing, which is a powerful library that makes it easy to work with audio in Python. And let\u0026rsquo;s not forget about the AI models used for the snarky responses - Gemini and Ollama are both cutting-edge models that can understand and respond to natural language input.\nUse Cases #\rBut how would you actually use this project in real life? Here are a few examples:\nLaunch your coding workspace with a voice command, so you can start working on your project right away. Get feedback on your code with text-to-speech responses, so you can stay focused on your work without having to constantly check your screen. Have fun with the snarky AI responses - because coding can be serious business, but it doesn\u0026rsquo;t have to be boring! Pro Tips or Implementation Details #\rSo, you want to get started with the project? Here are some pro tips to keep in mind:\nMake sure to install Python 3.11 and PyAudio before running the project. Configure your environment by setting up your .env file with your Gemini API key and workspace shortcut. Choose your AI model - do you want to use Gemini for online responses or Ollama for offline responses? If you\u0026rsquo;re using PowerToys, make sure to set up your workspace and create a shortcut to activate it. Some other things to keep in mind:\nYou\u0026rsquo;ll need to pull the Ollama model if you want to use it for offline responses. You can customize your wake phrase to whatever you like - just make sure it\u0026rsquo;s not too similar to other voice commands you use! Conclusion #\rSo, what are you waiting for? Head on over to the Jarvis-welcome-automation repo and start exploring the code! With this project, you\u0026rsquo;ll be able to launch your coding workspace with just a voice command, and get feedback on your code with text-to-speech responses. It\u0026rsquo;s like having your own personal assistant, right at your fingertips! Don\u0026rsquo;t be shy - give it a try and see what you can create!\nCode Highlights #\rCheck out this awesome code snippet from the project:\nvoice_assistant.py #\rimport speech_recognition as sr\rimport pyttsx3\rimport google.generativeai as genai\rimport requests\rimport subprocess\rfrom dotenv import load_dotenv\rimport os\rimport socket\r# Load environment variables\r// ... more code ... Project Details #\rGitHub Repository: Jarvis-welcome-automation Created: 2025-02-24 Language: Python ","date":"12 March 2025","externalUrl":null,"permalink":"/posts/2025-03-12-jarvis-welcome-automation/","section":"Posts","summary":"","title":"Jarvis Welcome Automation","type":"posts"},{"content":"\rBlog 2 Tweet Agent #\rHey there, tech enthusiast! Are you tired of manually sharing your blog posts on Twitter? You\u0026rsquo;re in luck! I\u0026rsquo;ve got something amazing to share with you - the Blog-2-Tweet-Agent, a game-changing GitHub project that turns your blog posts into engaging Twitter threads automatically! Can you imagine having more time to focus on what matters most - creating awesome content - while this bot does the heavy lifting for you?\nProject Overview #\rThe Blog-2-Tweet-Agent is an AI-powered bot that monitors your blog directory and instantly creates Twitter threads from new posts. It\u0026rsquo;s like having your own personal social media assistant! With this bot, you can effortlessly share your latest blog posts with your Twitter audience, complete with threaded tweets that are optimized for engagement and readability. But what makes this project so cool? Let\u0026rsquo;s dive in and find out!\nKey Features #\rHere are some of the standout features of the Blog-2-Tweet-Agent:\nAI-powered tweet generation using Google\u0026rsquo;s Gemini Automatic threaded tweets that are optimized for engagement and readability Real-time blog directory monitoring so you can stay on top of your latest posts Smart rate limiting and error handling to ensure that your tweets are posted smoothly and efficiently Dry run mode for testing, so you can see how the bot works without actually posting to Twitter Support for markdown files from your blog, making it easy to share your content Technical Highlights #\rSo, what makes this project technically impressive? For starters, the bot uses real-time monitoring to watch your blog directory for new posts, and instant processing to handle new markdown files as they arrive. It also features smart directory handling that works across different operating systems, and improved error handling to recover from API and file system issues. But don\u0026rsquo;t just take my word for it - the project\u0026rsquo;s v2.0 update brings a host of new features, including real-time monitoring, instant processing, and startup processing. What do you think - are you ready to take your Twitter game to the next level?\nUse Cases #\rSo, how might you use the Blog-2-Tweet-Agent in real life? Here are a few examples:\nAutomate your social media workflow: Use the bot to automatically share your latest blog posts on Twitter, saving you time and effort. Increase engagement: With threaded tweets that are optimized for engagement and readability, you can increase your Twitter engagement and reach a wider audience. Enhance your personal brand: By sharing your latest blog posts on Twitter, you can establish yourself as a thought leader in your industry and enhance your personal brand. Pro Tips or Implementation Details #\rTo get started with the Blog-2-Tweet-Agent, you\u0026rsquo;ll need to:\nClone the repository and set up your Python environment Configure your credentials by getting your Twitter API credentials and setting up your .env file Run the bot and start sharing your latest blog posts on Twitter! Some other tips to keep in mind:\nMake sure to test the bot in dry run mode before actually posting to Twitter Customize the bot to fit your needs by modifying the code and configuration files Monitor the bot\u0026rsquo;s performance to ensure that it\u0026rsquo;s working smoothly and efficiently Conclusion #\rSo, what are you waiting for? Head on over to the Blog-2-Tweet-Agent repository and start exploring the code! With its AI-powered tweet generation, real-time monitoring, and smart directory handling, this bot is the perfect addition to your social media workflow. Don\u0026rsquo;t miss out on this opportunity to take your Twitter game to the next level - try out the Blog-2-Tweet-Agent today and start sharing your latest blog posts with the world!\nProject Details #\rGitHub Repository: Blog-2-Tweet-Agent Created: 2025-02-14 Language: Python ","date":"26 February 2025","externalUrl":null,"permalink":"/posts/2025-02-26-blog-2-tweet-agent/","section":"Posts","summary":"","title":"Blog 2 Tweet Agent","type":"posts"},{"content":"\rJarvis Welcome Automation #\rHey there, tech enthusiast! Are you ready to take your productivity to the next level with a voice-activated workspace assistant? You\u0026rsquo;re in luck because I\u0026rsquo;m about to introduce you to an amazing GitHub project that\u0026rsquo;s going to change the way you work forever!\nProject Overview #\rImagine being able to launch your entire coding workspace with just a voice command! The Jarvis-welcome-automation project makes this a reality, using Python as its main language. This innovative tool is designed to make your life easier, saving you time and effort so you can focus on what really matters - writing amazing code!\nKey Features #\rHere are the standout features of this project:\nVoice activation with a customizable wake phrase - because who doesn\u0026rsquo;t love a little personalization? Launches your coding workspace via shortcut - no more tedious clicking or typing required! Snarky AI responses using Gemini (online) or Ollama (offline) - because a little humor never hurts, right? Text-to-speech feedback - so you can stay focused on your code without distractions! Technical Highlights #\rSo, what makes this project technically impressive? For starters, it uses PyAudio for voice recognition, which is a powerful library that allows for real-time audio processing. The project also utilizes Gemini and Ollama for AI responses, which provide a high level of accuracy and personalization. And let\u0026rsquo;s not forget about the customizable wake phrase - a feature that\u0026rsquo;s both convenient and secure!\nUse Cases #\rBut how might you use this project in real life? Here are a few examples:\nLaunch your coding workspace with a simple voice command, saving you time and effort. Stay focused on your code with text-to-speech feedback, minimizing distractions and maximizing productivity. Add some humor to your workday with snarky AI responses - because laughter is the best medicine, right? Pro Tips or Implementation Details #\rTo get the most out of this project, here are some pro tips:\nMake sure to install Python 3.11 for Windows and PyAudio using the provided wheel file. Configure your environment by copying .env.example to .env and setting your Gemini API key and WORKSPACE_SHORTCUT path. Consider installing Ollama for offline mode and pulling the Ollama model for optimal performance. Conclusion #\rSo, what are you waiting for? Head over to the Jarvis-welcome-automation repository and start exploring the code! With its innovative features and technical prowess, this project is sure to take your productivity to new heights! Don\u0026rsquo;t be shy - give it a try and experience the power of a voice-activated workspace assistant for yourself!\nCode Highlights #\rCheck out this awesome code snippet from the project:\nvoice_assistant.py #\rimport speech_recognition as sr\rimport pyttsx3\rimport google.generativeai as genai\rimport requests\rimport subprocess\rfrom dotenv import load_dotenv\rimport os\rimport socket\r# Load environment variables\r// ... more code ... Project Details #\rGitHub Repository: Jarvis-welcome-automation Created: 2025-02-24 Language: Python ","date":"26 February 2025","externalUrl":null,"permalink":"/posts/2025-02-26-jarvis-welcome-automation/","section":"Posts","summary":"","title":"Jarvis Welcome Automation","type":"posts"},{"content":"","date":"8 February 2025","externalUrl":null,"permalink":"/tags/blogging/","section":"Tags","summary":"","title":"Blogging","type":"tags"},{"content":"Okay, buckle up research enthusiasts! Tired of hitting paywalls and feeling like deep research is only for the elite with deep pockets? What if I told you there\u0026rsquo;s a way to unlock serious research power without spending a dime? Yep, you heard that right ‚Äì FREE! Get ready to meet DeepResearch, your new secret weapon for conquering complex questions. This isn\u0026rsquo;t your average, run-of-the-mill chatbot, folks. DeepResearch is a next-level, open-source framework that\u0026rsquo;s all about iterative reasoning. Think of it as your tireless AI sidekick, ready to scour the web, devour webpages, and piece together answers like a research ninja. Student, pro researcher, coding whiz, or just plain curious? DeepResearch, juiced up by the awesome Gemini API, is about to become your go-to for knowledge adventures! Here\u0026rsquo;s a little teaser of how it works: Introduction to the Framework: Your Free Research Powerhouse #\rDeepResearch isn\u0026rsquo;t just throwing shade at those pricey research tools ‚Äì it\u0026rsquo;s offering a whole new vibe:\nCha-Ching! (Or\u0026hellip; Not!): Seriously, it\u0026rsquo;s open-source and FREE. Wave goodbye to those crazy monthly fees and say hello to research access for everyone! Shape It Your Way: Being open-source means flexibility galore. Want to tweak it for your specific needs? Go for it! Want to add your own genius touch? The stage is yours! Privacy Perks: Even though it rocks the cloud-powered Gemini API, DeepResearch hands you more control than those mysterious, closed-box platforms. You get to see how it ticks and check its moves. Think of DeepResearch as the super-smart engine, and the Gemini API as the rocket fuel making it zoom!\nWhy Use the Gemini API? Supercharging DeepResearch #\rPairing DeepResearch with the Gemini API is like giving your research superpowers. Here‚Äôs the lowdown on why Gemini API is the dream team partner:\nBrainpower Unleashed: The Gemini API hooks you up with Google\u0026rsquo;s brainiest AI models, like the lightning-fast gemini-2.0-flash. Translation? DeepResearch gets smarter, answers get sharper. Handles the Heavy Lifting: Google\u0026rsquo;s cloud muscles mean DeepResearch can tackle massive research projects and mountains of info without breaking a sweat. Talk about reliable! Plays Well with Others: The Gemini API is built to play nice, so connecting it with DeepResearch is smooth sailing. You\u0026rsquo;ll be doing pro-level research in no time. Always in the Know: Cloud APIs like Gemini are generally plugged into the freshest info on the web, so your research is always cutting-edge. Basically, Gemini API injects DeepResearch with top-tier AI smarts, making it a pro at understanding tricky questions and whipping up awesome insights.\nStep-by-Step Guide: Integrating DeepResearch with Gemini API #\rReady to unleash DeepResearch with Gemini? Let\u0026rsquo;s get you set up in a few easy peasy steps:\nGear Up:\nAPI Keys are Your Magic Passwords: You\u0026rsquo;ll need keys for both Gemini and Jina Reader to power up DeepResearch. Think of them as VIP passes: Gemini API Key: Snag your free Gemini API key from Google AI Studio. This is your key to Gemini\u0026rsquo;s amazing reasoning skills. Jina API Key: Grab a free Jina API key with a sweet 1 million free tokens from jina.ai/reader. This lets DeepResearch zip through webpages and grab content using Jina Reader. Node.js \u0026amp; npm ‚Äì Get \u0026rsquo;em Installed: Make sure you\u0026rsquo;ve got Node.js and npm (Node Package Manager) on your machine. Head over to nodejs.org to download and install. Clone it Like a Pro (with Git):\nFire up your terminal and clone the DeepResearch code from GitHub. Don\u0026rsquo;t worry, it\u0026rsquo;s easier than it sounds:\ngit clone https://github.com/jina-ai/node-DeepResearch.git cd node-DeepResearch npm install Secret Agent Time: Environment Variables:\nLet\u0026rsquo;s keep those API keys safe and sound by setting them as environment variables. Pop these commands into your terminal, but swap in your actual keys for YOUR_GEMINI_API_KEY and jina_YOUR_JINA_API_KEY:\nexport GEMINI_API_KEY=YOUR_GEMINI_API_KEY export JINA_API_KEY=jina_YOUR_JINA_API_KEY First Test Drive!\nTime to see DeepResearch in action! Use the npm run dev command, followed by your burning question in quotes. Let\u0026rsquo;s kick things off with a simple one:\nnpm run dev \u0026#34;what is the latest blog post\u0026#39;s title from jina ai?\u0026#34; Boom! DeepResearch, powered by Gemini, gets to work. It\u0026rsquo;ll search, analyze, reason, and BAM ‚Äì the answer pops up right in your terminal.\nCrank Up the Complexity:\nDeepResearch really struts its stuff with tougher research puzzles. Try out some of the example questions from the Demo section in the README to see what it can really do. Like these:\nnpm run dev \u0026#34;what is the context length of readerlm-v2?\u0026#34; npm run dev \u0026#34;who will be the biggest competitor of Jina AI?\u0026#34; Watch DeepResearch go through its paces, exploring the web and thinking its way through to the answer. Pretty cool, huh?\nReal-World Applications and Use Cases: Deep Research in Action #\rDeepResearch, with Gemini in its corner, is ready for prime time. Let\u0026rsquo;s peek at a couple of real-world examples to spark your imagination:\nUse Case 1: Sniffing Out the Latest Company Buzz\nWant to stay in the loop with Jina AI? DeepResearch is on it:\nThe Query: Your mission: \u0026quot;what is the latest news from Jina AI?\u0026quot; Launch Time: Type this into your terminal and hit enter: npm run dev \u0026quot;what is the latest news from Jina AI?\u0026quot; DeepResearch Does Its Thing: It starts hunting for webpages all about \u0026ldquo;Jina AI news\u0026rdquo;. It dives into articles and blog posts that look promising, reading them carefully. Gemini kicks in, helping it make sense of everything, pinpoint the newest news, and pull out the juicy bits. The Big Reveal: DeepResearch delivers a neat summary of the latest news, maybe even with links to the original articles. Score! Use Case 2: Twitter Stalker (But for Research!)\nNeed to find the Twitter handle of Jina AI\u0026rsquo;s founder? DeepResearch is on the case:\nThe Query: Your target: \u0026quot;what is the twitter account of jina ai's founder\u0026quot; Engage! Run this command: npm run dev \u0026quot;what is the twitter account of jina ai's founder\u0026quot; DeepResearch, Private Eye: It searches for pages mentioning \u0026ldquo;Jina AI founder\u0026rdquo; and \u0026ldquo;Twitter account\u0026rdquo; ‚Äì like a digital detective. It checks out profiles and articles, looking for clues connecting Jina AI, its founder, and Twitter. Gemini helps it understand the context and nail down the founder\u0026rsquo;s Twitter handle with laser precision. Mission Accomplished: DeepResearch hands you the founder\u0026rsquo;s Twitter handle, ready for you to check out their Twitterverse. And that‚Äôs just scratching the surface! DeepResearch + Gemini API can tackle tons of research tasks, from market deep dives to competitor intel, from tech research to whipping up content, and a whole lot more.\nPro Tips for Maximum Research Power #\rWant to get the most out of DeepResearch and Gemini API? Here\u0026rsquo;s the inside scoop:\nBe Crystal Clear With Your Questions: The clearer your question, the faster and more focused DeepResearch will be. Think laser beam, not floodlight. Token Watch: Keep an eye on your token usage, especially for those epic research quests. Gemini API bills by tokens, and while Jina Reader\u0026rsquo;s free tier is generous with 1M tokens, it\u0026rsquo;s good to be mindful for big projects. Refine as You Go: For super complex questions, DeepResearch might take a few steps to get there. Watch the updates in your terminal to see how it\u0026rsquo;s thinking, and tweak your question if you need to guide it. Key Check! Double-check those Gemini and Jina API keys! Make sure they\u0026rsquo;re set up right as environment variables. Wrong keys = research roadblocks. Server Mode for the Pros: Want to hook DeepResearch into other apps or use it programmatically? Check out the Web Server API ‚Äì it\u0026rsquo;s all in the README. DeepResearch vs. The Big Guys: Open Source FTW! #\rLet\u0026rsquo;s stack up DeepResearch + Gemini API against those fancy, paid research tools. Here\u0026rsquo;s where the open-source magic shines:\nWallet Relief: DeepResearch is free as in beer (or puppies!). And Jina Reader\u0026rsquo;s free tokens are super generous. Say goodbye to subscription shock! Total Customization Freedom: Open-source means you can bend, twist, and mold DeepResearch to fit your exact research style. Try doing that with closed-source tools! You\u0026rsquo;re in Control: You\u0026rsquo;re the boss of your research and your data. Peek under the hood, see how it works, and know exactly what\u0026rsquo;s going on. Transparency for the win! Sure, those proprietary tools might have slick interfaces and all-in-one packages, but DeepResearch gives you power, customization, and cost savings ‚Äì especially if you dig open source and being in control.\nConclusion: The Future of Research is Open and Awesome! #\rDeepResearch, boosted by the Gemini API, is a giant leap for making deep research accessible to everyone. Now you can dive into serious, AI-powered research without those crazy subscription fees holding you back. Whether you need quick answers or want to explore the depths of complex topics, DeepResearch is your versatile, go-to research buddy.\nReady to dive into the open research revolution?\nClone the DeepResearch code now: https://github.com/jina-ai/node-DeepResearch Smash that Star button on GitHub! Show some love and stay in the loop! Share your research wins (and maybe funny fails!) with the community ‚Äì let\u0026rsquo;s learn together! Become a DeepResearch hero! Spot a bug? Got a cool feature idea? Want to contribute code? Jump in and help make DeepResearch even more amazing! Your deep research adventure starts today with DeepResearch and Gemini API ‚Äì knowledge power to the people!\n","date":"8 February 2025","externalUrl":null,"permalink":"/posts/deepresearch-but-open-source/","section":"Posts","summary":"","title":"DeepResearch BUT Open-Source???","type":"posts"},{"content":"","date":"8 February 2025","externalUrl":null,"permalink":"/tags/ditchclosedai/","section":"Tags","summary":"","title":"DitchClosedAI","type":"tags"},{"content":"Okay, so AI is EVERYWHERE, right? It\u0026rsquo;s changing how we work, learn, create ‚Äì you name it! I\u0026rsquo;m not gonna lie, I wanted my family to get in on this AI revolution too. Imagine my parents crushing it at work with super-powered research, and my little cousin sis becoming a creative genius with AI tools!\nBut here\u0026rsquo;s the thing that kept me up at night: privacy. Seriously, handing over all our family\u0026rsquo;s data to massive AI companies, especially ones based in places where governments can just demand access?\nNo, thanks!\nThat\u0026rsquo;s why I went full DIY and built a personal AI server right here at home. Boom! Total control over our data, zero worries about prying eyes. Plus, I get to be the cool tech guy who sets up a safe, kid-friendly AI zone for my cousin, and an unrestricted, power-user AI for my parents. Ready to ditch the privacy headaches and build your own awesome family AI server? Let\u0026rsquo;s dive into the WSL setup!\nHere are the steps, I followed, to bring this personal AI server to life:\nPrerequisites for WSL Installation: #\rEnable WSL: If you haven\u0026rsquo;t already, you need to enable WSL on your Windows machine. Open PowerShell as Administrator and run:\nwsl --install This command will install the default Ubuntu distribution. You may need to restart your computer after this step.\nUpdate WSL: After installation, ensure your WSL environment is up-to-date by opening your Ubuntu terminal (you can search for \u0026ldquo;Ubuntu\u0026rdquo; in the Start Menu) and running:\nsudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y **Step 1: Laying the Foundation with Ollama #\rWhat is Ollama? Think of Ollama as the engine that powers your AI models. Installing Ollama within WSL on Windows provides a performance advantage.\nInstallation in WSL: Open your Ubuntu terminal in WSL and use the following command to install Ollama:\ncurl -fsSL https://ollama.com/install.sh | sh This command downloads and executes the official Ollama installation script directly in your WSL environment.\nVerify Installation: After the script completes, you can verify Ollama is installed by running:\nollama --version This should display the installed Ollama version.\nPull the DeepSeek 1.5B Model: Immediately after verifying Ollama, pull the deepseek-1.5b model. This model is a capable and efficient language model you can start experimenting with. Run this command in your Ubuntu terminal:\nollama pull deepseek-r1:1.5b Ollama will download the deepseek-r1:1.5b model. The download time will depend on your internet connection.\n**Step 2: Creating a Web Interface with Open Web UI! #\rWhy Open Web UI? This provides a clean and intuitive web interface to interact with your AI models. It runs inside a Docker container.\nDocker Desktop Installation (Windows): You\u0026rsquo;ll need to install Docker Desktop on your Windows host machine (not inside WSL directly). Download and install Docker Desktop for Windows. Ensure that WSL 2 based engine is enabled in Docker Desktop settings (Settings -\u0026gt; General -\u0026gt; Use the WSL 2 based engine).\nDeploying Open Web UI in WSL: Open your Ubuntu terminal in WSL and use the following Docker command to deploy Open Web UI:\ndocker run -d -p 8080:8080 --name open-webui --volume open-webui:/app/data --network=host ghcr.io/open-webui/open-webui:main Let\u0026rsquo;s break down this command:\ndocker run -d: Runs the Docker container in detached mode (in the background). -p 8080:8080: Maps port 8080 on your host machine to port 8080 in the container, allowing you to access Open Web UI via http://localhost:8080 in your Windows browser. --name open-webui: Assigns the name \u0026ldquo;open-webui\u0026rdquo; to the container for easier management. --volume open-webui:/app/data: Creates a Docker volume named \u0026ldquo;open-webui\u0026rdquo; and mounts it to /app/data inside the container. This persists your Open Web UI data even if the container is removed. --network=host: This is important for WSL. Using host network mode allows the Docker container to directly use the host\u0026rsquo;s network, which simplifies networking with Ollama running in WSL. ghcr.io/open-webui/open-webui:main: Specifies the Docker image to use for Open Web UI. Accessing the Web UI: After Docker pulls the image and starts the container, navigate to http://localhost:8080 in your web browser (on Windows). You\u0026rsquo;ll be greeted with the Open Web UI interface.\nStep 3: Integrating Image Generation with Stable Diffusion #\rStable Diffusion and Automatic 1111: For AI image generation.\nPrerequisites in WSL: Open your Ubuntu terminal in WSL. We\u0026rsquo;ll use conda for Python environment management within WSL.\nsudo apt install curl git wget sudo apt install python3 python3-venv Install Automatic 1111 in WSL:\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui cd stable-diffusion-webui ./webui.sh git clone ...: Clones the Automatic 1111 repository. cd stable-diffusion-webui: Changes directory into the cloned repository. ./webui.sh: Executes the installation script. This will take a while as it downloads necessary components, including PyTorch and Stable Diffusion models, within your WSL environment. Accessing Stable Diffusion: Once the webui.sh script completes and starts, Stable Diffusion will be accessible through a web interface, on port 7860 (http://localhost:7860) in your Windows browser.\nConnecting Stable Diffusion to Open Web UI: In Open Web UI\u0026rsquo;s settings (accessed via http://localhost:8080), find the \u0026ldquo;images\u0026rdquo; section. Add the base URL of your Automatic 1111 installation. If you are running both Open Web UI and Automatic 1111 on the same machine (even with WSL), this will likely be http://localhost:7860.\nAlright! Follow these WSL steps, grab that deepseek-1.5b model, and just like that, you\u0026rsquo;ve got your own super-private AI server set up on your Windows machine, ready for the whole family to use! Just a heads-up ‚Äì if things feel a little sluggish, you might need to adjust the settings in Docker Desktop and WSL to give everything enough resources, especially if you\u0026rsquo;re running both Ollama and Stable Diffusion at the same time. Have fun exploring and see what amazing things you and your family can create together!\n","date":"7 February 2025","externalUrl":null,"permalink":"/posts/my-very-own-local-ai-server/","section":"Posts","summary":"","title":"My very own Local AI server!","type":"posts"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/posts/testing-hugo-posting/","section":"Posts","summary":"","title":"iS iT wORKING?","type":"posts"},{"content":"I decided that I will be starting this blog just so I can post more nerdy stuff than non-premium X.com account can handle. :P\nTools You‚Äôll Need #\rObsidian: My go-to note-taking app for drafting blog posts (Obsidian.md). Hugo: A static site generator for turning Markdown into a website (gohugo.io). Git/GitHub: For version control (optional, but useful for Vercel integration). Vercel: For hosting your Hugo site (vercel.com). Step 1: Organize Your Obsidian Vault #\rCreate a posts folder: This will store all your blog drafts in Obsidian. Also note where all your Obsidian directories are using the following steps: Note structure: Write posts in Markdown with Hugo-compatible frontmatter: --- title: \u0026#34;My First Blog Post\u0026#34; date: 2025-02-06 draft: false tags: - blogging - tech --- Step 2: Set Up Hugo #\rInstall Hugo #\rPrerequisites: Install Git and Go. Install Hugo globally: brew install hugo # Mac choco install hugo # Windows Create a new site: hugo new site my-blog \u0026amp;\u0026amp; cd my-blog git init Add a Theme (Terminal Theme Example) #\rYou can get the theme of your choice from Hugo Themes.\nInstall as a submodule: ## Initialize a git repository (Make sure you are in your Hugo website directory) git init ## Set global username and email parameters for git git config --global user.name \u0026#34;Your Name\u0026#34; git config --global user.email \u0026#34;your.email@example.com\u0026#34; ## Install a theme (we are installing the Terminal theme here). Once downloaded it should be in your Hugo themes folder git submodule add -f https://github.com/panr/hugo-theme-terminal.git themes/terminal Configure hugo.toml: Edit your hugo.toml file (using nano hugo.toml, notepad hugo.toml, or code hugo.toml) and add or update the configuration: baseurl = \u0026#34;/\u0026#34; languageCode = \u0026#34;en-us\u0026#34; theme = \u0026#34;terminal\u0026#34; [pagination] pagerSize = 5 [params] contentTypeName = \u0026#34;posts\u0026#34; showMenuItems = 2 showLanguageSelector = false fullWidthTheme = false centerTheme = false autoCover = true showLastUpdated = false [params.twitter] creator = \u0026#34;\u0026#34; site = \u0026#34;\u0026#34; [languages] [languages.en] languageName = \u0026#34;English\u0026#34; title = \u0026#34;Terminal\u0026#34; [languages.en.params] subtitle = \u0026#34;A simple, retro theme for Hugo\u0026#34; owner = \u0026#34;\u0026#34; keywords = \u0026#34;\u0026#34; copyright = \u0026#34;\u0026#34; menuMore = \u0026#34;Show more\u0026#34; readMore = \u0026#34;Read more\u0026#34; readOtherPosts = \u0026#34;Read other posts\u0026#34; newerPosts = \u0026#34;Newer posts\u0026#34; olderPosts = \u0026#34;Older posts\u0026#34; missingContentMessage = \u0026#34;Page not found...\u0026#34; missingBackButtonLabel = \u0026#34;Back to home page\u0026#34; minuteReadingTime = \u0026#34;min read\u0026#34; words = \u0026#34;words\u0026#34; [languages.en.params.logo] logoText = \u0026#34;Terminal\u0026#34; logoHomeLink = \u0026#34;/\u0026#34; [languages.en.menu] [[languages.en.menu.main]] identifier = \u0026#34;about\u0026#34; name = \u0026#34;About\u0026#34; url = \u0026#34;/about\u0026#34; [[languages.en.menu.main]] identifier = \u0026#34;showcase\u0026#34; name = \u0026#34;Showcase\u0026#34; url = \u0026#34;/showcase\u0026#34; Step 3: Sync Obsidian to Hugo #\rAutomate Content Transfer #\rWindows: Use robocopy to mirror your Obsidian posts folder to Hugo‚Äôs content/posts: robocopy \u0026#34;C:\\obsidian-vault\\posts\u0026#34; \u0026#34;C:\\hugo-blog\\content\\posts\u0026#34; /mir Mac/Linux: Use rsync for the same effect: rsync -av --delete ~/obsidian-vault/posts/ ~/hugo-blog/content/posts/ Step 4: Handle Images #\rPython Script for Image Relinking #\rRun this script to copy images from Obsidian‚Äôs attachment folder to Hugo‚Äôs static/images and fix Markdown links:\nimport os import re import shutil # Paths posts_dir = r\u0026#34;C:\\Users\\RANADEEP LASKAR\\Documents\\RanaBlog\\content\\posts\u0026#34; attachments_dir = r\u0026#34;C:\\Users\\RANADEEP LASKAR\\Documents\\Attachments\u0026#34; static_images_dir = r\u0026#34;C:\\Users\\RANADEEP LASKAR\\Documents\\RanaBlog\\static\\images\u0026#34; # Process each markdown file in the posts directory for filename in os.listdir(posts_dir): if filename.endswith(\u0026#34;.md\u0026#34;): filepath = os.path.join(posts_dir, filename) with open(filepath, \u0026#34;r\u0026#34;) as file: content = file.read() # Find all image links in the format [[image.png]] images = re.findall(r\u0026#39;\\[\\[([^]]*\\.png)\\]\\]\u0026#39;, content) # Replace image links and ensure URLs are correctly formatted for image in images: markdown_image = f\u0026#34;![Image Description](/images/{image.replace(\u0026#39; \u0026#39;, \u0026#39;%20\u0026#39;)})\u0026#34; content = content.replace(f\u0026#34;[[{image}]]\u0026#34;, markdown_image) # Copy the image to the Hugo static/images directory if it exists image_source = os.path.join(attachments_dir, image) if os.path.exists(image_source): shutil.copy(image_source, static_images_dir) # Write the updated content back to the markdown file with open(filepath, \u0026#34;w\u0026#34;) as file: file.write(content) print(\u0026#34;Markdown files processed and images copied successfully.\u0026#34;) Step 5: Deploy to Vercel #\rPrepare Your Repository (if not done earlier): Initialize Git in your Hugo site directory:\ngit init git add . git commit -m \u0026#34;Initial commit\u0026#34; Push Your Repository to GitHub (Optional): If you prefer integrating Vercel with GitHub for automatic deployments:\ngit remote add origin https://github.com/yourusername/your-repo.git git push -u origin main Deploy Using Vercel You have two options to deploy your Hugo site with Vercel:\nOption A: Deploy via the Vercel Dashboard #\rSign Up / Log In: Go to vercel.com and sign in with your GitHub.\nImport Your Project: Click on \u0026ldquo;New Project\u0026rdquo; and import your GitHub repository. Configure Build Settings:\nFramework Preset: Select \u0026ldquo;Hugo\u0026rdquo;. Output Directory: Set to public. Deploy: Click \u0026ldquo;Deploy\u0026rdquo;. Vercel will build your Hugo site and provide you with a live URL. Option B: Deploy via the Vercel CLI #\rInstall Vercel CLI (if not already installed):\nnpm install -g vercel Run Vercel in Your Project Directory:\nvercel Follow the interactive prompts to link or create a new Vercel project. Vercel will detect your Hugo site and build it accordingly.\nFor Production Deployments, run:\nvercel --prod This command will deploy your site to production and return the live URL.\nFinal Notes #\rPreview locally: Run hugo server -D to test your site locally. Automate Deployments: If you push your repository to GitHub and link it with Vercel, every push to your main branch will trigger a new deployment. Environment Variables \u0026amp; Custom Configs: You can further customize your deployment via the Vercel dashboard or by adding a vercel.json configuration file. Inspired by NetworkChuck‚Äôs blog pipeline. Simplified for Vercel hosting.\n","date":"6 February 2025","externalUrl":null,"permalink":"/posts/my-personal-growth-blog-pipeline/","section":"Posts","summary":"","title":"My Personal Growth Blog Pipeline","type":"posts"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/tag1/","section":"Tags","summary":"","title":"TAG1","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/tag2/","section":"Tags","summary":"","title":"TAG2","type":"tags"},{"content":"","date":"6 February 2025","externalUrl":null,"permalink":"/tags/tech/","section":"Tags","summary":"","title":"Tech","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]